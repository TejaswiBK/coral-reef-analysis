# Professional Standards: Coral Reef Analysis

Guidelines and best practices applied to the coral-reef-analysis repository.

---

## ðŸ“‹ Table of Contents

1. [Data Management Standards](#data-management-standards)
2. [Code Quality Standards](#code-quality-standards)
3. [Documentation Standards](#documentation-standards)
4. [Reproducibility Standards](#reproducibility-standards)
5. [Version Control Standards](#version-control-standards)
6. [Statistical Standards](#statistical-standards)
7. [Visualization Standards](#visualization-standards)

---

## Data Management Standards

### 1. ETL (Extract, Transform, Load) Pattern

The repository follows the professional ETL pattern with clear separation of concerns:

```
Script 1: EXTRACT & LOAD (Data Generation)
â”œâ”€ Generate raw data
â”œâ”€ Introduce realistic quality issues
â”œâ”€ Add quality flags (OK/SUSPECT/OUTLIER)
â””â”€ Save to data/raw/ (immutable reference)

Script 2: TRANSFORM & LOAD (Analysis)
â”œâ”€ Read raw data
â”œâ”€ Validate data quality
â”œâ”€ Clean and transform
â”œâ”€ Integrate datasets
â”œâ”€ Create analysis outputs
â””â”€ Document decisions (provenance log)
```

**Why:** Separation allows different teams to work independently, enables fast iteration, and maintains audit trail.

---

### 2. Data Immutability

#### Raw Data Files
- **Location:** `data/raw/`
- **Status:** Immutable (never modified after generation)
- **Version:** Generated once, used multiple times
- **Tracked in Git:** No (regenerated from script)
- **Purpose:** Permanent reference for all transformations

#### Processed Data Files
- **Location:** `data/processed/`
- **Status:** Generated from raw data (can be regenerated)
- **Version:** Derived, not original
- **Tracked in Git:** No (regenerated by analysis script)
- **Purpose:** Working dataset for analysis

**Principle:** Raw data is sacredâ€”never edit it directly. Always document transformations.

---

### 3. Data Quality Management

#### Quality Flags
Each dataset includes quality assessment:

```
OK       = Data passes all quality checks
SUSPECT  = Data within plausible range but unusual
OUTLIER  = Data fails statistical tests
```

#### Cleaning Decisions
All cleaning decisions are documented in:
- `data_provenance_log.csv` â€” Record-level decisions
- Console output â€” Detailed transformation messages
- Script comments â€” Why each decision was made

**Standard:** Retain 80-95% of records after cleaning. Flag and explain every removal.

---

### 4. Metadata Documentation

Each raw dataset includes metadata file with:

```
- Data source (simulated with realistic properties)
- Collection date/period
- Geographic coverage
- Variable definitions
- Units of measurement
- Known issues or limitations
- Quality assessment criteria
```

**Standard:** Complete metadata for every dataset. Future users understand data without asking.

---

## Code Quality Standards

### 1. Script Structure

Both generation and analysis scripts follow consistent structure:

```r
# ---- 1. SETUP ----
# Libraries, working directory, options
library(tidyverse)
set.seed(456)  # Reproducibility

# ---- 2. DATA INPUT ----
# Read data, validate structure
raw_data <- read_csv("data/raw/file.csv")

# ---- 3. DATA TRANSFORMATION ----
# Clean, transform, integrate
processed_data <- raw_data |>
  filter(quality_flag == "OK") |>
  mutate(...)

# ---- 4. ANALYSIS ----
# Statistical models, summaries
summary_table <- processed_data |>
  group_by(...) |>
  summarise(...)

# ---- 5. VISUALIZATION ----
# Plots, figures, graphics
ggplot(processed_data, ...) + ...

# ---- 6. OUTPUT ----
# Save results
write_csv(summary_table, "outputs/summary.csv")
```

**Principle:** Clear logical flow. Easy to follow, modify, and debug.

---

### 2. Naming Conventions

#### Files
```
generate_eco_socio_data_FIXED.R    # Data generation
analyze_eco_socio_data_FIXED.R     # Analysis
ecological_raw.csv                 # Raw data
combined_data_cleaned.csv          # Processed data
01_coral_trends.png                # Numbered outputs
data_provenance_log.csv            # Audit trail
```

**Standard:** Descriptive, snake_case, numbered outputs for ordering.

#### Variables
```r
# Clear, descriptive names
coral_cover           # Not: cc, coral_cov, cc_pct
fish_biomass          # Not: fish, bio, biomass_kg
fishing_effort        # Not: effort, fish_eff
region_name           # Not: reg, r
year_of_survey        # Not: y, yr, year
```

**Standard:** Full words, lowercase with underscores, clear meaning.

#### Functions
```r
# Verb_noun pattern
calculate_summary()           # Not: summary(), calc()
create_visualization()        # Not: plot(), make_plot()
validate_data_quality()       # Not: validate(), check()
```

**Standard:** Action-oriented, clearly indicates purpose.

---

### 3. Code Comments

#### Section Headers
```r
# ---- 1. READ AND VALIDATE DATA ----
# Clearly separate major sections
```

#### Inline Comments
```r
# Explain WHY, not WHAT
coral_cover <- pmax(coral_cover, 20)  # Keep realistic minimum

# Not: coral_cover <- pmax(coral_cover, 20)  # Set max value
```

#### Block Comments
```r
# Data quality assessment:
# - Remove records with >30% missing values
# - Flag values >3 SD from mean as outliers
# - Document all removals in provenance log
data_cleaned <- data_raw |>
  filter(missing_pct < 0.30) |>
  mutate(quality_flag = ...)
```

**Standard:** Comment the reasoning, not the syntax. Code should be readable; comments explain decisions.

---

### 4. Code Standards

#### Use Pipes (Tidyverse)
```r
# Good
data |>
  filter(year >= 2020) |>
  group_by(region) |>
  summarise(mean_value = mean(value))

# Avoid
summarise(
  group_by(
    filter(data, year >= 2020),
    region
  ),
  mean_value = mean(value)
)
```

#### Consistent Formatting
```r
# Spacing
x <- 5  # Good: spaces around operators
x<-5    # Avoid: no spaces

# Line length
# Keep lines <80 characters for readability

# Indentation
if (condition) {
  statement1  # Consistent 2-space indentation
  statement2
}
```

#### Error Handling
```r
# Document assumptions and constraints
if (!file.exists("data/raw/file.csv")) {
  stop("Raw data file not found. Run generate_data_FIXED.R first.")
}

# Check data structure
if (nrow(data) == 0) {
  warning("No data records found after filtering.")
}
```

**Standard:** Defensive coding. Check assumptions, provide clear error messages.

---

## Documentation Standards

### 1. README.md Structure

Each project README includes:

```
1. Project Overview        â€” What it is, why it matters
2. Quick Start            â€” Installation + first run
3. File Structure         â€” What files do what
4. Project Scope          â€” Geographic/temporal coverage
5. Key Findings           â€” Main results
6. Visualizations         â€” Description of plots
7. Tables                 â€” Summary outputs
8. Models                 â€” Statistical analysis
9. Customization          â€” How to modify
10. Troubleshooting       â€” Common problems
11. Getting Help          â€” Support resources
```

**Standard:** Comprehensive yet scannable. Users can find answers quickly.

---

### 2. QUICKSTART.md Structure

Quick reference for getting started:

```
1. Installation (2 minutes)        â€” Exact commands
2. Run Analysis (30 seconds)       â€” Copy-paste commands
3. Check Results                    â€” What to look for
4. Customize Examples               â€” How to modify
5. Common Issues                    â€” Troubleshooting
```

**Standard:** Minimal text, maximum actionable content. Users can run it in 5 minutes.

---

### 3. In-Code Documentation

#### Script Headers
```r
# ========================================
# PROJECT: Ecological + Socio-Economic Integration
# SCRIPT: Data Generation
# PURPOSE: Create realistic raw data with quality flags
# INPUT: None (synthetic generation)
# OUTPUT: ecological_raw.csv, socioeconomic_raw.csv, metadata.csv
# AUTHOR: Project Name
# DATE: December 2025
# ========================================
```

#### Section Documentation
```r
# ---- 1. GENERATE ECOLOGICAL DATA ----
# 
# Create ecological survey data for 5 regions, 2018-2023
# Introduces realistic patterns:
# - Coral decline over time (region-specific rates)
# - Fish biomass correlates with coral cover
# - Measurement variation (noise)
#
```

**Standard:** Complete context for every script. Future you will thank present you.

---

## Reproducibility Standards

### 1. Random Seed Management

```r
# Set seed at the start of data generation
set.seed(456)  # Fixed seed = reproducible results

# All subsequent random operations produce identical results
rnorm(10)      # Same 10 numbers every time
sample(1:100)  # Same sample every time
```

**Standard:** Every data generation includes fixed seed. Results are identical across computers/users.

---

### 2. Dependency Documentation

#### Explicit Requirements
```r
# At top of every script
library(tidyverse)    # Version 2.0.0+
library(sf)           # Version 1.0.0+
library(terra)        # Version 1.7.0+
library(viridis)      # Any recent version
library(patchwork)    # Version 1.1.0+
library(scales)       # Version 1.1.0+
```

#### Version Checking (Optional)
```r
# Verify minimum versions
if (packageVersion("tidyverse") < "2.0.0") {
  stop("tidyverse version 2.0.0 or higher required")
}
```

**Standard:** Clear statement of requirements. Users install correct versions.

---

### 3. File Path Management

#### Relative Paths (Portable)
```r
# Good: Works from any location
read_csv("data/raw/ecological_raw.csv")
write_csv(result, "outputs/summary.csv")

# Avoid: Only works from specific directory
read_csv("/Users/name/Projects/coral-reef/data/raw/...")
write_csv(result, "C:/Users/name/coral-reef/outputs/...")
```

**Standard:** Relative paths allow scripts to work on any computer. Users don't need to edit paths.

---

### 4. Session Reproducibility

```r
# Record session information
sessionInfo()  # Shows R version and all package versions

# Can be called at end of script to document environment
```

---

## Version Control Standards

### 1. What to Commit

```
âœ“ DO commit:
  - R scripts (*.R)
  - Documentation (*.md)
  - Configuration (.gitignore)
  - Project files (README, LICENSE)

âœ— DON'T commit:
  - Data files (*.csv, *.tif)
  - Generated outputs (*.png, *.pdf)
  - R environment (.Rhistory, .Rdata)
  - IDE files (.Rproj, .vscode/)
```

---

### 2. Commit Messages

#### Good Commit Messages
```
# Format: [Type] Brief description

[FEATURE] Add visualization for eco-socio integration
[FIX] Correct data cleaning logic in analysis script
[DOCS] Update QUICKSTART guide with new examples
[REFACTOR] Simplify statistical model code
[STYLE] Format code to improve readability
```

#### Bad Commit Messages
```
update
fixed
work in progress
changes
```

**Standard:** Clear, descriptive, actionable commit messages. Browse history tells the story.

---

### 3. Branching Strategy (Optional)

For team projects:
```
main          â€” Production-ready code
develop       â€” Integration branch
feature/*     â€” Individual features
bugfix/*      â€” Bug fixes
docs/*        â€” Documentation updates
```

**Standard:** Clean git history. Easy to understand changes.

---

## Statistical Standards

### 1. Model Reporting

Every statistical model includes:

```r
# Full model summary
summary(model1)

# Extract key results
cat("Coefficient:", coef(model1)[1], "\n")
cat("R-squared:", summary(model1)$r.squared, "\n")
cat("P-value:", summary(model1)$coefficients[1, 4], "\n")

# Assumptions checking (optional)
plot(model1)  # Diagnostic plots
```

**Standard:** Report full summaries, not just coefficients. Show assumptions are met.

---

### 2. Interpretation Guide

```r
# Always include interpretation, not just numbers
cat("Interpretation: A 1-unit increase in X is associated with",
    round(coef(model)["X"], 3), "unit change in Y\n")
```

**Standard:** Numbers alone are meaningless. Explain what they mean.

---

### 3. Multiple Comparisons

When multiple models:
```r
# Compare models
anova(model1, model2)

# Report which is better and why
cat("Model 2 is better: lower AIC by", 
    AIC(model1) - AIC(model2), "\n")
```

**Standard:** Explicitly compare and justify choice of final model.

---

## Visualization Standards

### 1. Plot Quality

#### Color Palettes
```r
# Use colorblind-friendly palettes
scale_color_viridis_d()      # Discrete
scale_color_viridis_c()      # Continuous

# Avoid
scale_color_brewer(palette = "Spectral")  # Less accessible
```

#### Labels and Titles
```r
# Complete information on every plot
ggplot(...) +
  labs(
    title = "Coral Cover Declining Across All Regions",
    subtitle = "2018-2023 ecological surveys",
    x = "Year",
    y = "Coral Cover (%)",
    color = "Region",
    caption = "Data: Ecological surveys 2018-2023"
  )
```

#### Readability
```r
# Readable text sizes and styles
theme(
  axis.text = element_text(size = 11),
  axis.title = element_text(size = 12, face = "bold"),
  legend.position = "bottom"
)
```

**Standard:** Publication-ready plots. Clear labels, accessible colors, professional appearance.

---

### 2. File Format and Resolution

```r
# Save as PNG for web
ggsave("outputs/plot.png", width = 10, height = 6, dpi = 300)

# Or PDF for printing
ggsave("outputs/plot.pdf", width = 10, height = 6)
```

**Standard:** High resolution (300 DPI) for publication. Multiple formats for different uses.

---

### 3. Visualization Documentation

```r
# Each plot includes interpretation in comments
# Example: What does this plot show?
# - X axis: Time period (2018-2023)
# - Y axis: Coral cover percentage
# - Color: Different regions
# - Interpretation: Coral declining fastest in Southern Red Sea
```

**Standard:** Users understand what plot shows and why it matters.

---

## Data Integrity Standards

### 1. Validation Checks

```r
# Verify expected structure
stopifnot(nrow(data) == 30)
stopifnot(all(c("region", "year", "coral_cover") %in% names(data)))

# Check value ranges
stopifnot(all(data$coral_cover >= 0 & data$coral_cover <= 100))
stopifnot(all(data$year >= 2018 & data$year <= 2023))
```

**Standard:** Assert expectations. Fail fast with clear error messages.

---

### 2. Completeness Reporting

```r
# Document completeness at each stage
cat("Input records:", nrow(data_raw), "\n")
cat("After cleaning:", nrow(data_clean), "\n")
cat("Retention rate:", round(100 * nrow(data_clean) / nrow(data_raw), 1), "%\n")
```

**Standard:** Every transformation reports what was kept/removed and why.

---

## Summary: The Five Principles

1. **Separation:** Data generation separate from analysis
2. **Immutability:** Raw data never changes
3. **Transparency:** Every decision documented
4. **Reproducibility:** Fixed seeds, relative paths, clear dependencies
5. **Professionalism:** Publication-ready quality standards

---

## Checklist for New Projects

When adding new projects to the repository, follow this checklist:

- [ ] Two separate scripts (generation + analysis)
- [ ] Fixed random seed in generation script
- [ ] Clear section headers in all scripts
- [ ] Complete README.md with all sections
- [ ] QUICKSTART.md with copy-paste commands
- [ ] Documentation of data quality decisions
- [ ] Provenance log of data transformations
- [ ] Publication-ready visualizations
- [ ] Statistical models with interpretations
- [ ] .gitignore configured correctly
- [ ] All relative paths (no absolute paths)
- [ ] Comments explaining WHY decisions were made

---

## References

### Best Practices
- Hadley Wickham - R for Data Science
- Jenny Bryan - Reproducible Research Best Practices
- DataCamp - Data Science Practices

### Tools & Standards
- tidyverse style guide
- Posit (RStudio) guides
- GitHub Guides

---

**Last Updated:** December 30, 2025  
**Status:** Professional Quality Standard
